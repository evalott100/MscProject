\documentclass[../Project.tex]{subfiles}
\begin{document}
\newpage
\section{Background}
\subsection{Group Theory}
	\begin{defi}[Conjugacy class \cite{2}]
		For elements $g,h \in G$. $g$ is conjugate to $h$ if $\exists x \in G$ such that $h = x^{-1}gx$. The conjugacy class of $g$ is the equivalence class $g^G = \Brace{x^{-1}gx \;\vert\; x \in G}$.
	\end{defi}

	\begin{prop}
		Given two conjugacy classes $g^G,h^G$ in $G$, either $g^G = h^G$ or $g^G \cap ^G = \varnothing$.
	\end{prop}
	\begin{proo*}
		TODO
	\end{proo*}
	\begin{defi}[Distinct conjugacy classes]
		 If $g^G \cap h^G = \varnothing$ then we say $g^G$ and $h^G$ are distinct.
	\end{defi}
	\begin{defi}[Representatives of conjugacy classes \cite{2}]
		Given distinct conjugacy classes $g_1^G,\dots,g_n^G$ such that $G = g_1^G\cup \cdots \cup g^G_n$, we call $g_1,\dots,g_n$ representatives of the conjugacy classes of $G$.
	\end{defi}

\newpage
\subsection{The Linear Algebra Recap and the General Linear Group}

\begin{theo}
	Given a projection $\pi$ of $V$ onto a subspace $W$, we have $V = \Ker(\pi) \oplus \Im(\pi)$.
	\label{projtheo}
\end{theo}

\begin{defi}[Trace of a matrix]
	The trace of a matrix $A \in \M_n(F)$ is the sum of the diagonal elements $\Tr(A) = \sum\limits_{i = 1}^nA^i_i$.
\end{defi}

\begin{theo}
	If $V$ is a finite dimensional vector space over an algebraically closed field $F$, and $L : V \to V$ is a linear map, then $L$ has at least one eigenvector.
	\label{eigenvectheo}
\end{theo}

\begin{prop}
\label{7}
	Let $A,B \in \M_n(F)$. Then
	\begin{itemize}
		\item $\Tr(A + B) = \Tr(A) + \Tr(B)$,
		\item $\Tr(AB) = \Tr(BA)$.
	\end{itemize}

	Also, if $T$ is invertable then $\Tr(T^{-1}AT) = \Tr(A)$.
\end{prop}

\begin{defi}[Endomorphisms and Automorphisms]
	Given a space $V$ with an algebraic structure, an endomorphism is defined to be a homomorphism from $V$ to itself. We denote $\End(V) \coloneqq \Hom(V,V)$ to be the set of all endomorphisms on $V$. An automorphism is an endomorphism which is also an isomorphism, and we denote $\Aut(V) \coloneqq \Brace{\phi \in \End(V)\;|\;\phi \text{ is an isomorphism}}$ as the set of all automorphisms on $V$.\\

	In this project, we focus on vector space endomorphisms - linear maps from a space to itself.
\end{defi}

\begin{defi}[Projection \cite{2}]
	Linear map $\pi$ from $V$ to a subspace $W$ is called a projection if and only if it satisfies $\pi^2 = \pi$, $\Im(\pi) = W$, $\pi\vert_W = \Id_W$.
\end{defi}




\begin{defi}[General linear group \cite{1}]
	Let $V$ be a vector space. We define
	$$\GL(V) \coloneqq \Aut(V)$$
	to be the set of invertable linear endomorphisms over $V$. We prove this is a group under composition.\label{1}
\end{defi}

\begin{proo*}
	\textbf{Associativity:} Composition is always associative.\\
	\textbf{Existance of inverse elements:} $\phi$ an isomorphism $\iff$ $\phi$ invertible. Hence every element of $\Aut(V)$ has an inverse.\\
	\textbf{Closedness:} The composition of linear maps is linear, and the composition of bijective maps is bijective. Therefore $\Aut(V)$ is closed under composition.\\
	\textbf{Existence of identity:} The identity map is linear and bijective, hence in $\Aut(V)$. $\blacksquare$
\end{proo*}

\begin{prop}
	 If $V$ is an $n$-dimensional vector space over $\mathbb{C}$ then there is a group isomorphism
	$$\GL(V) \cong \GL_n(\mathbb{C}) \coloneqq \Brace{A \in \M_n(\mathbb{C}) \;|\; A \text{ is invertable}},$$
	the group of invertable $n \times n$ matrices.
\end{prop}
\begin{proo*}
	Let $V$ be an $n$-dimensional vector space over $\mathbb{C}$ and fix a basis $e = \Brace{e_1,\dots,e_n}$. Recall that the result of a linear transformation is entirely determined by its result on basis elements (once a basis is chosen). Then for $L : V \to V$, we can write the result of $L$ on basis element $e_k$ as $L(e_k) = \alpha^1_ke_1 + \cdots + \alpha^n_ke_n$. Let $\phi : \Aut(V) \to \M_n(\mathbb{C})$ such that
	$$\phi(L) =
	\begin{pmatrix} 
	    \alpha^1_1 & \alpha^1_2 & \dots & \alpha^1_n \\
	    \alpha^2_1 & \alpha^2_2 & \cdots & \alpha^2_n \\
	    \vdots & \vdots& \ddots & \vdots\\
	    \alpha^n_1 & \alpha^n_2  &\dots & \alpha^n_n 
	\end{pmatrix}.$$
	Any matrix has a corresponding linear map which sends the $k$th basis vector to another vector with the basis coefficients made up of the scalars in the $k$th column. Also, since any linear map is determined by the vectors that the basis elements are mapped to, there is a unique linear map with columns as the coefficients. Then $\phi$ is a bijection.\\

	Now we show that $\phi$ is an homomorphism. Given $L_1(e_k) = \alpha^1_ke_1 + \cdots + \alpha^n_ke_n$ and $L_2(e_k) = \beta^1_ke_1 + \cdots + \beta^n_ke_n$, we have
	\begin{align*}
	L_2 \circ L_1(e_k) &= L_2(\alpha^1_ke_1 + \cdots + \alpha^n_ke_n) = \alpha_k^1L_2(e_1) + \cdots + \alpha_k^nL_2(e_n)\\
	&= \alpha^1_k(\beta^1_1e_1 + \cdots + \beta^n_1e_n) + \cdots + \alpha^n_k(\beta^1_ne_1 + \cdots + \beta^n_ne_n)\\
	&= e_1(\alpha^1_k\beta^1_1 + \cdots + \alpha^n_k\beta^1_n) + \cdots e_n(\alpha^1_k\beta^n_1 + \cdots +\alpha^n_k\beta^n_n).
	\end{align*}
	Therefore
	\begin{align*}
		\phi(L_2 \circ L_1) &= 
		\begin{pmatrix} 
	    (\alpha^1_1\beta^1_1 + \cdots +\alpha^n_1\beta^1_n) & (\alpha^1_2\beta^1_1 + \cdots + \alpha^n_2\beta^1_n) & \dots & (\alpha^1_n\beta^1_1 + \cdots +\alpha^n_n\beta^1_n) \\
	    (\alpha^1_1\beta^2_1 + \cdots +\alpha^n_1\beta^2_n) & (\alpha^1_2\beta^2_1 + \cdots + \alpha^n_2\beta^2_n) & \cdots & (\alpha^1_n\beta^2_1 + \cdots +\alpha^n_n\beta^2_n) \\
	    \vdots & \vdots& \ddots & \vdots\\
	    (\alpha^1_1\beta^n_1 + \cdots +\alpha^n_1\beta^n_n) & (\alpha^1_2\beta^n_1 + \cdots + \alpha^n_2\beta^n_n) &\dots & (\alpha^1_n\beta^n_1 + \cdots +\alpha^n_n\beta^n_n) 
		\end{pmatrix}\\
		&= \begin{pmatrix} 
	    \beta^1_1 & \beta^1_2 & \dots & \beta^1_n \\
	    \beta^2_1 & \beta^2_2 & \cdots & \beta^2_n \\
	    \vdots & \vdots& \ddots & \vdots\\
	    \beta^n_1 & \beta^n_2  &\dots & \beta^n_n 
	\end{pmatrix} \times
\begin{pmatrix} 
	    \alpha^1_1 & \alpha^1_2 & \dots & \alpha^1_n \\
	    \alpha^2_1 & \alpha^2_2 & \cdots & \alpha^2_n \\
	    \vdots & \vdots& \ddots & \vdots\\
	    \alpha^n_1 & \alpha^n_2  &\dots & \alpha^n_n 
	\end{pmatrix}
		= \phi(L_2) \times \phi(L_1). \;\blacksquare
	\end{align*}
\end{proo*}

\begin{defi}[Unitary map]
	A linear map $L : V \to W$ between inner product spaces $V,W$ is said to be unitary if and only if $\langle v_1, v_2\rangle  = \langle L(v_1), L(v_2) \rangle\;\forall v_1,v_2 \in V$.
	We denote the unitary maps of a vector space $V$ as $\U(V)$, and for maps over an $n$-dimensional vector space over $\mathbb{C}$, we have $U(V) \cong U_n(\mathbb{C}) \coloneqq \Brace{A \in \M_n(\mathbb{C}) \;\vert\; A^* = A^{-1}}$ where $A^*$ is the standard conjugate transpose.
\end{defi}

We will denote the invertable elements of a ring $R$ as $R^*$, then $\GL_1(\mathbb{C}) = \mathbb{C}^*$. Then $\mathbb{C}^* = \mathbb{C}\backslash \{0\}$ and not the dual space of $\mathbb{C}$ as is traditional.

\begin{exam}[\cite{1}]
	For the maps $\GL_1(\mathbb{C}) = \mathbb{C}^*$, a complex number $z$ is unitary if $\bar{z} = z^1 \implies z\bar{z} = \abs{z}^2 = 1 \implies z \in \mathbb{T}$, where $\mathbb{T} = \Brace{z \in \mathbb{C}\;\vert\;\abs{z} = 1}$ is the unit circle. Then $U_1(\mathbb{C}) = \mathbb{T}$
\end{exam}

\begin{theo}[Caley-Hamilton \cite{1}]
	Let $A$ be a matrix with characteristic polynomial $p_A(x)$. Then $p_A(A) = 0$.
\end{theo}

\begin{defi}[Minimal polynomial \cite{1}]
	For an endomorphism $A \in \End(V)$, the minimal polynomial of $A$, $m_A(x)$ is the smallest degree monic polynomial $f(x)$ such that $f(A) = 0$
\end{defi}

\begin{theo}[\cite{1}]
	A matrix $A \in \M_n(\mathbb{C})$ is diagonalizable if and only if all factors of $m_A(x)$ have multiplicity $1$.
\end{theo}

\begin{theo}[Spectral theorem \cite{1}]
	For a self adjoint $A \in \M_n(\mathbb{C})$, there exists a unitary matrix $U \in U_n(\mathbb{C})$ such that $U^* AU$ is diagonal. The eigenvalues of $A$ are real.
\end{theo}

TODO prove
\end{document}
